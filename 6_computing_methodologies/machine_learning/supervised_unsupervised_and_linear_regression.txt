Two major types of Machine Learning Algorithms

  1. Supervised Learning
    - Machine learning algorithm in which the "right answers" are given to the machine
        (i.e. The dataset already has defined in all the possible answers)
        - Discrete list of the possible values (Yes/No; 1, 2 , 5; Non lethal, Lethal, Fatal) is
        given to the machine

        Two subtypes
            - Regression algorithms: Predict a continous value output(example of the price of a house)
            - Classification algorithms: Predict a discrete value output(1 or 0; Yes,No,Maybe)

    - The task of the machine is to keep finding more of these "right answers"


    1.1. SUPERVISED LEARNING ALGORITHM PROCESS

        - Training set (x -> y) {m}
        - Supervised Learning Algorithm (Process {m}) to generate Hypothesis function {h}
        - {h}(x,y) -> Tries to predicate {y} for {x}
            [Based on the knowledge encoded into the training set] : The "Right Answers"

        Representation of h?

        hθ(x) = θ + θx
        : All this means is that we are going to predict that {y} is a linear function of {x}
        -> {y} is some straight line function of {x}
        -> This is a linear function f(x) = a + bx

      ∴ This model's name is "Linear Regression with one variable"
      "Linear Regression": Predict real-valued output {y}
      "With one Variable": Based on one input {x}

    1.2. Implementation of the "Linear Regression with one variable" model

    1.2.1. The "Cost Function"
      -> Figure out how to fit "the best straight line" through our data : Why though?
      ∴ How to implement (construct) this straight line

    1.2.2  hθ(x) = θ1 + θ2x
      -> θ1 and θ2 = parameters of the model

    1.2.3. How do you choose these parameters?

        Given {
          Given a dataset plotted(scatter plot) on cartesian plane
          Best straight line for the data identified
        }

        -> How do we determine the values of θ1 and θ2 that will give us this line?

          hθ(x) := Output of function
          y := Actual value

          Goal: Choose θ1 and θ2 so that {hθ(x)} is as close to {y} as possible for our
          training set

          ∴ Minimize the difference between hθ(x) and y

            Solve minimization problem:

            minimize (θ1, θ2)  (hθ(x) - y )^2

  2. Unsupervised Learning

    - Machine is tasked with finding the "right answer"
    - Find the Structure of the data and provide output to "make sense" of the data

    Types:
      - Clustering algorithms
      - Cocktail party algorithms


LINEAR REGRESSION WITH ONE VARIABLE (UNIVARIATE LINEAR REGRESSION)

  1. Regression problems:
    - take input variables
    - map output onto
        "continous" expected result function

  2. Univariate regression: Used to determine a
    - single output
    - from single input

    THE REGRESSION PROBLEM SOLVING PROCESS

      1. The hypothesis function

          hθ(x) = θ0 + θ1x

      2. Test the accuracy of the function (General equation to test accuracy)
         We can measure the accuracy of the hypothesis function using a "cost function"

         2.1. Squared error function (Mean squared error)
             The average of the
                - Results of the hypothesis function
                    Compare to the
                - Actual values (expected values)

                J(θ0,θ1) = 1/2m ∑m i=1 ( hθ( x(i) ) −y(i) )2 : It's application to our hypothesis function (h0)


      3. Gradient Descent
          A way to automatically improve  the hypothesis function

          Iterative improvement function using derivatives.
          We keep moving down the the slope.

          1. Derivative (the line tangent to a function) of the cost function
              Get the Gradient at a point
          2. Make steps down the derivative by α( The learning rate)

          3. Success: When cost function is at its minimum value

          Gradient Descent Equation:

              Repeat Until Conversion (Reach the minimum):{
                θj := θj − α ( ∂/∂θj  J(θ0,θ1) )
                for j = 0 and j = 1
              }

            Alternative:
            repeat until convergence:
              θj := θj − α [Slope of tangent aka derivative]


      4. Gradient Descent for Linear Regression

        * Use Gradient descent to MINIMIZE J(θ0,θ1)[The cost function]
        * Determine partial derivative's value for θ0 and θ1
          https://www.coursera.org/learn/machine-learning/supplement/Mc0tF/linear-regression-with-one-variable
          : Just the slop of the cost function  J(θ0,θ1)
