def 1: a word's meaning is given⁵ 
    by the words that
        frequently appear close by
    i.e. the meaning is given/derived from other words.
        Q: How?

def 2: meanining of a word is represented by the context⁶
    within which a word occurs.
    def context = the words close by/words around it
                    within a 
                        fixed size window

Example:
    the boy ate an apple for lunch
    it is summer the apple tree is beginning to bloom
    An apple a day keeps the doctor away

    apple's meaning = all the words around it across all the sentences
                        i.e. all its contexts
∴ build a representation of word w is from its many contexts c

- actual representation = word vector⁷
    large(50+ dimension) vector to represent one word and its meaning
    Example:
        50 dimensions(randomly generated)
        [0.55, 0.54, -0.96, -0.24, 0.44, 0.41, 0.82, 0.93, 0.91, 0.19, -0.84, 0.05, -0.62, -0.91, 0.15, -0.55, 0.23, 0.45, -0.58, 0.57, 0.44, -0.4, 0.65, 0.26, 0.85, -0.74, 0.42, -0.23, 0.88, -0.59, 0.51, -0.47, 0.95, -0.44, -0.86, -0.42, 0.67, -0.18, -0.65, 0.37, 0.43, -0.48, 0.63, -0.13, 0.09, 0.24, 0.17, -0.52, 0.96, 0.52]  

    properties:
        - called dense vector(all numbers non zero)
        - similar to vectors of other words that appear in the same context
            NB: Q: Is this where we get the similarity of words?
                    i.e. solve the similarity problem from one-hot vectors?
            O: So if a word is used in a similar way(i.e same context) as another then 
                    then those words are similar?
                The promixity of the word's vector in the vector space determine their similarity.
                    - The smaller the distance between them, the similar they are.
                    - The larger the distance between them, the more different they are.
            Example:
                Expanding the Apple example:
                    the boy ate an Orange for lunch
                    it is summer the Orange tree is beginning to bloom
                    An Orange a day keeps the doctor away
                    ∴ Apple is similar to Orange?

    word vectors = word embeddings = word representations.
        They are a distributed representation.
        Q: What does this mean?
    
    vectors plotted in n dimension vector space
        n = vector dimension choosen


references: 
    5,6,7 Chris Manning, CS224n: Natural Language Processing with Deep Learning,Stanford / Winter 2022, https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbFhrcnpCRDJsbVhXMFdFX2FROGRmbnk5YVBfQXxBQ3Jtc0ttUnRIT0pZQ0Uxa1NBaUVxd3ZRLVo0TzN2RTFLcUpWX2FsODBXTHVaaGVveHJUMVRaaUFldG9TcHdwU0c3QWlmeFJ4Ti1UOGZlZkMxZi1FTF82QWp6OF9sVUhEcHJyaG1KX203NElzSk9IUlJvQ1RxUQ&q=https%3A%2F%2Fstanford.io%2F3CORGu1&v=8rXD5-xhemo